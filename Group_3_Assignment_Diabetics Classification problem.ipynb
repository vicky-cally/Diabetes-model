Diabetics project using Machine learning algorithms which includes k-nearest neighbours (KNN), support vector machines (SVM), decision trees (DT) and random forest(RF)
Review on all the ML Algorithms listed above

Detailed Explanation of KNN

K-Nearest Neighbours is a machine learning algorithm used for both classification and regression problems. For a classification problem, KNN makes prediction based on the majority class why for a regression problem, it calculates the mean of it's nearest neighbour values.

How it works

In KNN, you assign a data point which is classified on the basis of it's K-nearest neighbours. To determine the value of K, the number of neighbors to consider, make use of cross-validation or other model evaluation techniques. Assuming k=3, You need the 3 nearest neighbours from the data point assigned by calculating the distance from the data point to all other data points(training data) using distance metrics. Distance metrics can be Euclidean distance, manhanttan distance and minkowski distance. The class that occurs the most among the K-nearest neighbours is assigned to the input data point.You can look at the image below.For a Regression problem, calculate the mean of the target values.

KNN%202.png

Advantages of KNN

It is easy to understand and implement.
It is applicable to both classification and regression problems.
It doesn't make assumptions about the underlying data distribution.
It doesn't require a training phase as it doesn't create an explicit model
Diadvantages and challenges of KNN

The computing cost of identifying neighbors increases as the dataset gets larger.
Scaling or normalizing the data is frequently required because KNN is sensitive to feature scale.
KNN performance tends to deteriorate in high-dimensional conditions.
In conclusion, KNN is an adaptable and comprehensible algorithm that can be applied to a range of applications, particularly those involving small to medium big datasets. It is a helpful tool in machine learning because of its simplicity, absence of assumptions, and adaptability. However, the dataset properties, the distance measure, and the choice of K can all have an influence on its performance.



Detailed Explanation of SVM

Effective for both regression and classification applications, Support Vector Machine (SVM) is a supervised machine learning technique. The aim of SVM is to locate the hyperplane in feature space that best divides the data into different categories. The main goal of support vector machines (SVM) is to maximize the margin between classes while minimizing classifcation errors. The support vectors are the data points that are closest to the decision boundary. For a regression problem, The objective of SVR is to locate a hyperplane that, within a certain margin, captures as much of the data as feasible.

How it works

Assuming we have a dataset with each data point represented by a set of features, incase of a binary classification, it can be 0 or 1. SVM aims to find a hyperplane that maximizes the margin between classes which is characterized by a weight vector (w) and bias (b) that minimizes the norm of the weight vector. The distance between the hyperplane and the closest data point from each class is called the margin, and it is this distance that SVM aims to optimize. The data points required for defining the ideal hyperplane are called support vectors. The diagram below shows how SVM works.

svm.png

Advantages

SVM is appropriate for applications involving a large number of features because it works effectively in high-dimensional feature spaces.
It can be used for both classification and regression tasks.
It is less likely to overfit, particularly in areas with several dimensions.
It can handle non-linear relationships in the data because of the kernel technique.
Disadvantages and challenges

SVM training can need a lot of processing power, particularly for large datasets.
Experimentation may be necessary to select the ideal kernel and fine-tune the hyperparameters.
Compared to simpler models, the decision boundaries generated by SVM may be more difficult to understand.
In conclusion, support vector machines are strong, adaptable algorithms that perform well in a range of machine learning applications. They work especially effectively with high-dimensional datasets and situations where there is a clear class distinction. To get the best performance, parameters and kernels must be chosen carefully.



Detailed Explanation of DT A prevalent supervised machine learning technique for both regression and classification applications is the decision tree. It creates a tree-like structure with internal nodes representing decisions based on features, branches representing decisions' outcomes, and leaf nodes representing the final predicted outcome by recursively partitioning the data into subsets based on the values of input features.In machine learning, decision trees offer simplicity and a visual representation of the possibilities when formulating outcomes.

How it works

Assuming we are trying to do a binary classification of 0 or 1, decision tree will classify this by dividing the dataset recursively according to the input feature values. It will then select the characteristic that best divides the data into homogenous groups at each stage, maximizing a splitting criteria (e.g., mean squared error or Gini impurity). This technique generates a structure that resembles a tree until a halting requirement is satisfied. In regression, it's the mean of the target values; in classification, it's the projected result for each leaf node's majority class. Overfitting can be avoided by pruning. The resultant tree offers a simple to understand decision-making approach.

DT.PNG

Advantages

Decision trees are simple to comprehend and analyze
Regarding the data's underlying distribution, decision trees don't make any assumptions.
Data with non-linear connections can be captured using decision trees.
Disadvantages and challenges

Overfitting of decision trees can occur, particularly when improper pruning occurs.
The structure of the tree can alter significantly in response to small changes in the data.
Decision trees may exhibit bias towards dominating classes in classification problems.
In conclusion, decision trees are popular machine learning models that are flexible and understandable. They provide interpretability and are appropriate for a variety of tasks. Pruning is one method for enhancing generalization performance, although caution must be used to prevent overfitting.



Detailed Explanation of RF The Random Forest algorithm is an ensemble learning technique that enhances overall accuracy and robustness by merging the predictions of several decision trees. Both regression and classification problems make extensive use of it. It determines the result by using the decision trees' predictions. By calculating the mean or average of the output from different trees, it makes predictions. The precision of the result increases with the number of trees.

How it works The way Random Forest operates is by bootstrapping (sampling with replacement) numerous decision trees from the original dataset and randomly selecting features at each split. The final prediction is more reliable and accurate than that of a single decision tree since the different trees average predictions for regression or vote for categorization. This method minimizes overfitting, effectively manages noise, and offers valuable insights into the significance of features. Because of its overall mechanism, Random Forest is an effective and adaptable ensemble learning method that may be applied to a variety of machine learning applications.Below is the pictorial representation.

RF%20image.PNG

Advantages

Compared to individual decision trees, Random Forest minimizes overfitting by employing several trees and randomization.
Random Forest yields better results than a single decision tree, particularly when dealing with complex datasets.
Random Forest can assess the importance of different features in making predictions.
Random Forest is resistant to data noise and anomalies.
Random Forest works well with datasets that have varying measurement units since it is insensitive to the size of the features.
Diadvantages and challenges

When compared to individual decision trees, Random Forest models might be more difficult to understand.
It can take a lot of computing power to train many decision trees, particularly for huge datasets.
In conclusion, Random Forest is an effective ensemble learning method that takes use of decision trees' advantages while resolving their drawbacks. Random Forest is an excellent choice for a range of machine learning applications because it offers great accuracy and robustness through the construction of many trees and the introduction of randomness.


STEPS TAKEN:

Data Acquisition

In order to make use of these algorithms, we employed a Diabetics dataset from Kaggle.

Summary of the dataset:

The data used for this analysis was collected and provided by the "National Institute of Diabetes and Digestive and Kidney Diseases" as part of the Pima Indians Diabetes Database. The dataset comprises instances from a larger database, with specific constraints. All patients included in the dataset belong to the Pima Indian heritage, which is a subgroup of Native Americans. Furthermore, all individuals in the dataset are females aged 21 and above.

In this project, we built classifcation models using K-Nearest Classifier, Support Vector Classifier, Decision Tree Classifier and Random Forest Classifier.

Importing the libraries

!pip install scikit-learn
Requirement already satisfied: scikit-learn in c:\users\admin\anaconda3\lib\site-packages (1.4.1.post1)
Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\users\admin\anaconda3\lib\site-packages (from scikit-learn) (1.26.4)
Requirement already satisfied: scipy>=1.6.0 in c:\users\admin\anaconda3\lib\site-packages (from scikit-learn) (1.11.4)
Requirement already satisfied: joblib>=1.2.0 in c:\users\admin\anaconda3\lib\site-packages (from scikit-learn) (1.2.0)
Requirement already satisfied: threadpoolctl>=2.0.0 in c:\users\admin\anaconda3\lib\site-packages (from scikit-learn) (2.2.0)
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler,StandardScaler

from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,confusion_matrix,classification_report
from sklearn.model_selection import cross_val_score, KFold,GridSearchCV
from sklearn.model_selection import GridSearchCV
dataset=pd.read_csv("diabetes2.csv")
df=dataset.copy()
df
Pregnancies	Glucose	BloodPressure	SkinThickness	Insulin	BMI	DiabetesPedigreeFunction	Age	Outcome
0	6	148	72	35	0	33.6	0.627	50	1
1	1	85	66	29	0	26.6	0.351	31	0
2	8	183	64	0	0	23.3	0.672	32	1
3	1	89	66	23	94	28.1	0.167	21	0
4	0	137	40	35	168	43.1	2.288	33	1
...	...	...	...	...	...	...	...	...	...
763	10	101	76	48	180	32.9	0.171	63	0
764	2	122	70	27	0	36.8	0.340	27	0
765	5	121	72	23	112	26.2	0.245	30	0
766	1	126	60	0	0	30.1	0.349	47	1
767	1	93	70	31	0	30.4	0.315	23	0
768 rows Ã— 9 columns

Meaning of the features and their importance in determining suffering from diabetics

Features	Meaning and Importance in determining the risk of suffering from diabetics
1. Pregnancy	The more the number of pregnacies, the more the risk of suffering from diabetics due to changes in insulin resistance during pregnancy
2. Glucose	High level glucose can cause diabetics
3. Blood pressure	Hypertension(High blood pressure) is associated with diabetics
4. Skin thickness	It has potential correlation with insulin resistance but not a direct measure
5. Insulin	High level of insulin might indicate insulin resistance and low insulin levels may also be observed in some cases of diabetics
6. BMI(Body Mass Index)	Elevated BMI is often associated with an increased risk of diabetics
7. Diabetics Pedigree Function	Provides a measure of diabetics hereditary(family history)
8. Age	The prevalence of diabetics increases with age
2. Data Cleaning

# Getting information of all tyhe features
df.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 768 entries, 0 to 767
Data columns (total 9 columns):
 #   Column                    Non-Null Count  Dtype  
---  ------                    --------------  -----  
 0   Pregnancies               768 non-null    int64  
 1   Glucose                   768 non-null    int64  
 2   BloodPressure             768 non-null    int64  
 3   SkinThickness             768 non-null    int64  
 4   Insulin                   768 non-null    int64  
 5   BMI                       768 non-null    float64
 6   DiabetesPedigreeFunction  768 non-null    float64
 7   Age                       768 non-null    int64  
 8   Outcome                   768 non-null    int64  
dtypes: float64(2), int64(7)
memory usage: 54.1 KB
#Checking for null values
df.isnull().sum()
Pregnancies                 0
Glucose                     0
BloodPressure               0
SkinThickness               0
Insulin                     0
BMI                         0
DiabetesPedigreeFunction    0
Age                         0
Outcome                     0
dtype: int64
# Statistical description of the data
df.describe()
Pregnancies	Glucose	BloodPressure	SkinThickness	Insulin	BMI	DiabetesPedigreeFunction	Age	Outcome
count	768.000000	768.000000	768.000000	768.000000	768.000000	768.000000	768.000000	768.000000	768.000000
mean	3.845052	120.894531	69.105469	20.536458	79.799479	31.992578	0.471876	33.240885	0.348958
std	3.369578	31.972618	19.355807	15.952218	115.244002	7.884160	0.331329	11.760232	0.476951
min	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.078000	21.000000	0.000000
25%	1.000000	99.000000	62.000000	0.000000	0.000000	27.300000	0.243750	24.000000	0.000000
50%	3.000000	117.000000	72.000000	23.000000	30.500000	32.000000	0.372500	29.000000	0.000000
75%	6.000000	140.250000	80.000000	32.000000	127.250000	36.600000	0.626250	41.000000	1.000000
max	17.000000	199.000000	122.000000	99.000000	846.000000	67.100000	2.420000	81.000000	1.000000
Findings:

There are no null values in the dataset.
From the statistical description, we can see that some columns like Glucose, BloodPressure, SkinThickness, Insulin, and BMI contain 0 as the minimum values, which is impossible. We have to convert the zeros to null values and fill or drop them.
The Pregnancy minimum value can be 0 because it indicates women who have never been pregnant.
3. Data Pre-processing and manipulation

# Calculating the number of zeroz in those columns
zero_columns = ["Pregnancies", "Glucose", "BloodPressure", "SkinThickness", "Insulin", "BMI"]

zeros_count = (df[zero_columns] == 0).sum()
print("Number of Zeros in Each Column:")
print(zeros_count)
Number of Zeros in Each Column:
Pregnancies      111
Glucose            5
BloodPressure     35
SkinThickness    227
Insulin          374
BMI               11
dtype: int64
# Replacing the 0 with nan and filling the null values with mean
affected_columns = ["Glucose", "BloodPressure", "SkinThickness", "Insulin", "BMI"]
df[affected_columns] = df[affected_columns].replace(0, np.nan)
df.fillna(df.mean(), inplace=True)
# If the person has ever been pregnant(>0), let it be equal to zero
df['Pregnancies'].values[df['Pregnancies'] >1]=1
df.describe()
Pregnancies	Glucose	BloodPressure	SkinThickness	Insulin	BMI	DiabetesPedigreeFunction	Age	Outcome
count	768.000000	768.000000	768.000000	768.000000	768.000000	768.000000	768.000000	768.000000	768.000000
mean	0.855469	121.686763	72.405184	29.153420	155.548223	32.457464	0.471876	33.240885	0.348958
std	0.351857	30.435949	12.096346	8.790942	85.021108	6.875151	0.331329	11.760232	0.476951
min	0.000000	44.000000	24.000000	7.000000	14.000000	18.200000	0.078000	21.000000	0.000000
25%	1.000000	99.750000	64.000000	25.000000	121.500000	27.500000	0.243750	24.000000	0.000000
50%	1.000000	117.000000	72.202592	29.153420	155.548223	32.400000	0.372500	29.000000	0.000000
75%	1.000000	140.250000	80.000000	32.000000	155.548223	36.600000	0.626250	41.000000	1.000000
max	1.000000	199.000000	122.000000	99.000000	846.000000	67.100000	2.420000	81.000000	1.000000
# Shuffling the data
df=df.sample(frac=1)
4. Exploratory Data Analysis

# Counting the number of 0 and 1 in the outcome column
df.Outcome.value_counts()
Outcome
0    500
1    268
Name: count, dtype: int64
Here the outcome dependent variable is binary where '0' mean 'non-diabetic' and '1' means 'diabetic'
The non-diabetic class is about 65% of the dataset and is way more than the diabetic class which is 35% which means there is a possibility that the model may be biased towards the majority class.
#Plot of the count

import matplotlib.pyplot as plt

outcome_labels = {0: 'Non-Diabetic', 1: 'Diabetic'}

plt.figure(figsize=(6, 4))
df['Outcome'].value_counts().plot(kind='bar', color=['green', 'yellow'])
plt.title('Outcome Counts')
plt.xlabel('Outcome')
plt.ylabel('Count')


plt.xticks(ticks=[0, 1], labels=[outcome_labels[0], outcome_labels[1]], rotation=0)

plt.show()

# Pairplot also indicating the binary classes

import seaborn as sns
import matplotlib.pyplot as plt


outcome_labels = {0: 'Non-Diabetic', 1: 'Diabetic'}

df['Outcome_Label'] = df['Outcome'].map(outcome_labels)


sns.pairplot(df, hue='Outcome_Label')


df = df.drop('Outcome_Label', axis=1)

plt.show()
C:\Users\Admin\anaconda3\Lib\site-packages\seaborn\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
  with pd.option_context('mode.use_inf_as_na', True):
C:\Users\Admin\anaconda3\Lib\site-packages\seaborn\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
  with pd.option_context('mode.use_inf_as_na', True):
C:\Users\Admin\anaconda3\Lib\site-packages\seaborn\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
  with pd.option_context('mode.use_inf_as_na', True):
C:\Users\Admin\anaconda3\Lib\site-packages\seaborn\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
  with pd.option_context('mode.use_inf_as_na', True):
C:\Users\Admin\anaconda3\Lib\site-packages\seaborn\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
  with pd.option_context('mode.use_inf_as_na', True):
C:\Users\Admin\anaconda3\Lib\site-packages\seaborn\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
  with pd.option_context('mode.use_inf_as_na', True):
C:\Users\Admin\anaconda3\Lib\site-packages\seaborn\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
  with pd.option_context('mode.use_inf_as_na', True):
C:\Users\Admin\anaconda3\Lib\site-packages\seaborn\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
  with pd.option_context('mode.use_inf_as_na', True):
C:\Users\Admin\anaconda3\Lib\site-packages\seaborn\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
  with pd.option_context('mode.use_inf_as_na', True):
C:\Users\Admin\anaconda3\Lib\site-packages\seaborn\axisgrid.py:118: UserWarning: The figure layout has changed to tight
  self._figure.tight_layout(*args, **kwargs)

#Relationship between the features and the outcome

correlation_matrix=df.corr()
print(correlation_matrix['Outcome'])
Pregnancies                 0.005705
Glucose                     0.492928
BloodPressure               0.166074
SkinThickness               0.215299
Insulin                     0.214411
BMI                         0.311924
DiabetesPedigreeFunction    0.173844
Age                         0.238356
Outcome                     1.000000
Name: Outcome, dtype: float64
Values close to 1 indicate a strong positive correlation, Values close to -1 indicate a strong negative correlation and Values close to 0 indicate a weak or no correlation.

Features	Meaning of their correlation
1. Pregnancies	0.005705 shows a weak positive correlation btw the number of pregnancies and the likelihood to the positive outcome
2. Glucose	0.492928 shows a moderate positive correlation btw the glucose levels and the likelihood to the positive outcome
3. BloodPressure	0.166074 shows a weak positive correlation btw the blood pressure and the likelihood to the positive outcome
4. SkinThickness	0.215299 shows a very weak positive correlation btw the skin thickness and the likelihood to the positive outcome
5. Insulin	0.214411 shows a very weak positive correlation btw the insulin levels and the likelihood to the positive outcome
6. BMI	0.311924 shows a moderate positive correlation btw the Body Mass Index and the likelihood to the positive outcome
7. DiabetesPedigreeFunction	0.173844 shows a weak positive correlation btw the diabetes pedigree function and the likelihood to the positive outcome
8. Age	0.238356 shows a weak positive correlation btw the age and the likelihood to the positive outcome
9. Outcome	1.000000 represents the correlation of the "Outcome" variable with itself, which is always perfect and equal to 1.0
#PLotting the correlation matrix

plt.figure(figsize=(7, 5))
sns.heatmap(df.corr(),annot=True, cmap='viridis', fmt=".2f") #mask=mask
plt.title('Correlation Matrix')
plt.show()

From the correlation matrix, we can see that we have moderate correlation of 0.54 between BMI and Skin thickness.
There is also correlation between Age and Blood pressure with 0.32 and Insulin and glucose with 0.42.
There is no negative correlation between the outcome and all the features.
# Plotting the relationships between some of the features

plt.figure(figsize=(20, 15))

plt.subplot(3, 3, 1)
sns.scatterplot(data=df, x="BMI", y="SkinThickness", hue="Outcome", palette={0: 'seagreen', 1: 'tomato'})
plt.title('BMI Vs Skinthickness')

plt.subplot(3, 3, 2)
sns.scatterplot(data=df, x="BloodPressure", y="Age", hue="Outcome", palette={0: 'purple', 1: 'deeppink'})
plt.title('Age Vs BloodPressure')

plt.subplot(3, 3, 3)
sns.scatterplot(data=df, x="Glucose", y="Insulin", hue="Outcome", palette={0: 'royalblue', 1: 'darkgoldenrod'})
plt.title('Insulin Vs Glucose')

plt.show()

5. Data Modelling

# Seperating the columns into x and y
x=df.drop(['Outcome'],axis=1)
y=df['Outcome']
Data Splitting

#Splitting the data using the 80, 20 split
xtrain,xtest,ytrain,ytest=train_test_split(x,y, 
                                           test_size=0.20,random_state=42)
Data Scaling

#Scaling the data using MinMaxScaler
scaler= StandardScaler()
xtrain=scaler.fit_transform(xtrain)
xtest=scaler.fit_transform(xtest)
# Checking the shape of xtrain,xtest,ytrain and ytest
xtrain.shape,xtest.shape,ytrain.shape,ytest.shape
((614, 8), (154, 8), (614,), (154,))
6. Model Building with the ML Algorithmns


model_knn = KNeighborsClassifier(metric='manhattan', n_neighbors=20, weights= 'distance')
model_knn.fit(xtrain, ytrain)
KNeighborsClassifier(metric='manhattan', n_neighbors=20, weights='distance')
In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.
On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.
# Checking the parameters in knn
model_knn.get_params()
{'algorithm': 'auto',
 'leaf_size': 30,
 'metric': 'manhattan',
 'metric_params': None,
 'n_jobs': None,
 'n_neighbors': 20,
 'p': 2,
 'weights': 'distance'}
model_knn_predictions = model_knn.predict(xtest)
train_predictions = model_knn.predict(xtrain)

knn_accuracy_test=accuracy_score(ytest, model_knn_predictions.round())
knn_accuracy_train=accuracy_score(ytrain, train_predictions.round())
precision=precision_score(ytest,model_knn_predictions.round()) #,average='weighted')
recall=recall_score(ytest, model_knn_predictions.round()) #,average='weighted')
f1score=f1_score(ytest, model_knn_predictions.round()) #,average='weighted')

print('KNN_test Accuracy:%.2f%%' %(knn_accuracy_test*100.0))
print('KNN_train Accuracy:%.2f%%' %(knn_accuracy_train*100.0))
print('KNN Precision:%.2f%%' %(precision*100.0))
print('KNN Recall:%.2f%%' %(recall*100.0))
print('KNN F1-score:%.2f%%' %(f1score*100.0))
KNN_test Accuracy:71.43%
KNN_train Accuracy:100.00%
KNN Precision:53.06%
KNN Recall:55.32%
KNN F1-score:54.17%
report = classification_report(ytest, model_knn_predictions, output_dict=True)

sns.heatmap(pd.DataFrame(report).iloc[:-1, :].T, annot=True, cmap="Pastel1")
plt.title("Precision, Recall, F1-Score for the KNN Algorithmn")
plt.show()

The precision of the diabetic class is 0.76 which means that 76% of those the model projected to have diabetes really had the disease.
The recall of the diabetic class is 0.67 which means that 67% of all diabetics were properly predicted by the model.
F1-score of the diabetic class is 0.71 which implies a balance of precision and recall.
conf_matrix_knn = confusion_matrix(ytest, model_knn_predictions)
sns.heatmap(conf_matrix_knn, annot=True, fmt='d', cmap='cividis')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for K-Nearest Classifier')
plt.show()

This confusion matrix shows that the model predicted 93 0's correctly and misclassified 16 as 1.
The model predicted 29 1's correctly and misclassified 16 as 0.
# Actual and predicted values in a dataframe
dg=pd.DataFrame({'Actual Values':ytest[30:60],'Predicted Values_knn':model_knn_predictions[30:60]})
dg = dg.reset_index(drop=True)
dg.head(5)
Actual Values	Predicted Values_knn
0	0	0
1	0	0
2	0	0
3	0	0
4	1	0
Hyperparameter tuning for k-nearest neighbours

train_accuracy = []
test_accuracy = []

neighbors_range = range(1, 21)

for n_neighbors in neighbors_range:
    knn_model = KNeighborsClassifier(n_neighbors=n_neighbors)
    knn_model.fit(xtrain, ytrain)

    train_accuracy.append(knn_model.score(xtrain, ytrain))
    test_accuracy.append(knn_model.score(xtest, ytest))

plt.figure(figsize=(8, 4))
plt.plot(neighbors_range, train_accuracy, label='Training Accuracy', marker='o', color='black')
plt.plot(neighbors_range, test_accuracy, label='Testing Accuracy', marker='o', color='red')
plt.xlabel('Number of Neighbors')
plt.ylabel('Accuracy')
plt.title('Training and Testing Accuracy vs. Number of Neighbors')
plt.legend()
plt.grid(True)
plt.show()

best_accuracy = 0
best_neighbors = 0

for i in range(len(neighbors_range)):
    if test_accuracy[i] > best_accuracy:
        best_accuracy = test_accuracy[i]
        best_neighbors = neighbors_range[i]


print(f"The number of neighbors with the highest test accuracy is: {best_neighbors}")
The number of neighbors with the highest test accuracy is: 12
# Trying hyperparameter tuning using grid search
grid_params = { 'n_neighbors' : [13,15,19,21,23,25],
               'weights' : ['uniform','distance'],
               'metric' : ['minkowski','euclidean','manhattan']}
gs = GridSearchCV(KNeighborsClassifier(), grid_params, verbose = 1, cv=10, n_jobs = -1)
g_res = gs.fit(xtrain, ytrain)
print(f'The best score is ',g_res.best_score_)
print(f'The best parameters are ',g_res.best_params_)
Fitting 10 folds for each of 36 candidates, totalling 360 fits
The best score is  0.7898730830248546
The best parameters are  {'metric': 'minkowski', 'n_neighbors': 21, 'weights': 'distance'}
g_res.best_params_
{'metric': 'minkowski', 'n_neighbors': 21, 'weights': 'distance'}


model_svm= SVC(C= 10, gamma = 'auto',kernel = 'rbf')
model_svm.fit(xtrain,ytrain)
SVC(C=10, gamma='auto')
In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.
On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.
#Checking the parameters in the svm
model_svm.get_params()
{'C': 10,
 'break_ties': False,
 'cache_size': 200,
 'class_weight': None,
 'coef0': 0.0,
 'decision_function_shape': 'ovr',
 'degree': 3,
 'gamma': 'auto',
 'kernel': 'rbf',
 'max_iter': -1,
 'probability': False,
 'random_state': None,
 'shrinking': True,
 'tol': 0.001,
 'verbose': False}
model_svm_predictions=model_svm.predict(xtest)
svm_train_prediction=model_svm.predict(xtrain)

svm_accuracy_test=accuracy_score(ytest, model_svm_predictions.round())
svm_accuracy_train=accuracy_score(ytrain,svm_train_prediction.round())
precision=precision_score(ytest, model_svm_predictions.round()) 
recall=recall_score(ytest, model_svm_predictions.round()) 
f1score=f1_score(ytest, model_svm_predictions.round()) 

print('SVM_Accuracy_test:%.2f%%' %(svm_accuracy_test*100.0))
print('SVM_Accuracy_train:%.2f%%' %(svm_accuracy_train*100.0))
print('Precision:%.2f%%' %(precision*100.0))
print('Recall:%.2f%%' %(recall*100.0))
print('F1-score:%.2f%%' %(f1score*100.0))
SVM_Accuracy_test:70.13%
SVM_Accuracy_train:87.79%
Precision:50.91%
Recall:59.57%
F1-score:54.90%
report = classification_report(ytest, model_svm_predictions, output_dict=True)

sns.heatmap(pd.DataFrame(report).iloc[:-1, :].T, annot=True, cmap="Pastel1")
plt.title("Precision, Recall, F1-Score for the SVM Algorithmn")
plt.show()

conf_matrix_svm = confusion_matrix(ytest, model_svm_predictions)
sns.heatmap(conf_matrix_svm, annot=True, fmt='d', cmap='inferno')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for Support Vector Classifier')
plt.show()

dh=pd.DataFrame({'Actual Values':ytest[30:60],'Predicted Values_svm':model_svm_predictions[30:60]})
dh = dh.reset_index(drop=True)
dh.head(3)
Actual Values	Predicted Values_svm
0	0	0
1	0	1
2	0	0
Hyperparameter tuning for support vector machines

#Checking for the accuracy of train and test using different values of C
C_values = [0.1, 1, 10, 100]

train_accuracy = []
test_accuracy = []

for C_value in C_values:
    svm_model = SVC(C=C_value)
    svm_model.fit(xtrain, ytrain)
    train_accuracy.append(svm_model.score(xtrain, ytrain))
    test_accuracy.append(svm_model.score(xtest, ytest))
    


plt.figure(figsize=(8, 4))
plt.plot(C_values, train_accuracy, label='Training Accuracy', marker='o', color='purple')
plt.plot(C_values, test_accuracy, label='Testing Accuracy', marker='o', color='hotpink')
plt.xscale('log')  # Use log scale for C values, as they may vary widely
plt.xlabel('C (Regularization Parameter)')
plt.ylabel('Accuracy')
plt.title('Training and Testing Accuracy vs. C (SVM)')
plt.legend()
plt.grid(True)
plt.show()

 

model_dt= DecisionTreeClassifier(criterion= 'entropy',max_depth=5, min_samples_split=5)
model_dt.fit(xtrain,ytrain)
model_dt.get_params()
{'ccp_alpha': 0.0,
 'class_weight': None,
 'criterion': 'entropy',
 'max_depth': 5,
 'max_features': None,
 'max_leaf_nodes': None,
 'min_impurity_decrease': 0.0,
 'min_samples_leaf': 1,
 'min_samples_split': 5,
 'min_weight_fraction_leaf': 0.0,
 'monotonic_cst': None,
 'random_state': None,
 'splitter': 'best'}
model_dt_prediction=model_dt.predict(xtest)
model_dt_predict=model_dt.predict(xtrain)
DT_accuracy_test=accuracy_score(ytest, model_dt_prediction.round())
DT_accuracy_train=accuracy_score(ytrain, model_dt_predict.round())
precision=precision_score(ytest, model_dt_prediction.round()) 
recall=recall_score(ytest, model_dt_prediction.round()) 
f1score=f1_score(ytest, model_dt_prediction.round()) 

print('DT_Accuracy_test:%.2f%%' %(DT_accuracy_test*100.0))
print('DT_Accuracy_train:%.2f%%' %(DT_accuracy_train*100.0))
print('Precision:%.2f%%' %(precision*100.0))
print('Recall:%.2f%%' %(recall*100.0))
print('F1-score:%.2f%%' %(f1score*100.0))
DT_Accuracy_test:77.27%
DT_Accuracy_train:81.76%
Precision:65.00%
Recall:55.32%
F1-score:59.77%
print(ytest.shape)
print(model_dt_prediction.shape)
print(model_dt_predict.shape)
(154,)
(154,)
(614,)
print(f'classification_report for Decision tree:\n{classification_report(ytest, model_dt_prediction)}')
classification_report for Decision tree:
              precision    recall  f1-score   support

           0       0.82      0.87      0.84       107
           1       0.65      0.55      0.60        47

    accuracy                           0.77       154
   macro avg       0.73      0.71      0.72       154
weighted avg       0.77      0.77      0.77       154

conf_matrix_dt = confusion_matrix(ytest, model_dt_prediction)
sns.heatmap(conf_matrix_dt, annot=True, fmt='d', cmap='plasma')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for Decision Tree Classifier')
plt.show()

di=pd.DataFrame({'Actual Values':ytest[30:60],'Predicted Values_dt':model_dt_prediction[30:60]})
di = di.reset_index(drop=True)
di.head(5)
Actual Values	Predicted Values_dt
0	0	0
1	0	0
2	0	0
3	0	0
4	1	1
Hyperparameter tuning for decision tree

# Define the Decision Tree classifier
dt_classifier = DecisionTreeClassifier()

# Define the hyperparameter grid for grid search
param_grid = {'criterion': ['gini', 'entropy'], 'max_depth': [None, 5, 10, 15], 'min_samples_split': [2, 5, 10]}

# Perform grid search with cross-validation
grid_search = GridSearchCV(dt_classifier, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)
grid_search.fit(xtrain, ytrain)

# Get the best hyperparameters from the grid search
best_params_dt = grid_search.best_params_
print("Best Hyperparameters for Decision Tree:", best_params_dt)

# Train the Decision Tree classifier with the best hyperparameters
best_dt_classifier = DecisionTreeClassifier(**best_params_dt)
best_dt_classifier.fit(xtrain, ytrain)

# Make predictions on the test set
y_pred_dt = best_dt_classifier.predict(xtest)

# Evaluate the accuracy of the model
accuracy_dt = accuracy_score(ytest, y_pred_dt)
print("Test Accuracy for Decision Tree:", accuracy_dt)
Fitting 5 folds for each of 24 candidates, totalling 120 fits
Best Hyperparameters for Decision Tree: {'criterion': 'gini', 'max_depth': 15, 'min_samples_split': 10}
Test Accuracy for Decision Tree: 0.6948051948051948

model_rf= RandomForestClassifier(criterion='entropy', max_depth=15, min_samples_leaf=4, 
                                 min_samples_split=5, n_estimators=100)
model_rf.fit(xtrain, ytrain)

#'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 100
#criterion='entropy',max_depth=10,min_samples_leaf=4, min_samples_split=10, n_estimators=50
RandomForestClassifier(criterion='entropy', max_depth=15, min_samples_leaf=4,
                       min_samples_split=5)
In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.
On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.
model_rf_prediction=model_rf.predict(xtest)
model_rf_train=model_rf.predict(xtrain)
RF_accuracy_test=accuracy_score(ytest, model_rf_prediction.round())
RF_accuracy_train=accuracy_score(ytrain, model_rf_train.round())
precision=precision_score(ytest, model_rf_prediction.round()) 
recall=recall_score(ytest, model_rf_prediction.round()) 
f1score=f1_score(ytest, model_rf_prediction.round()) 

print('RF_Accuracy_test:%.2f%%' %(RF_accuracy_test*100.0))
print('RF_Accuracy_train:%.2f%%' %(RF_accuracy_train*100.0))
print('RF_Precision:%.2f%%' %(precision*100.0))
print('Recall:%.2f%%' %(recall*100.0))
print('F1-score:%.2f%%' %(f1score*100.0))
RF_Accuracy_test:76.62%
RF_Accuracy_train:92.51%
RF_Precision:62.22%
Recall:59.57%
F1-score:60.87%
print(f'Classification report for Random Forest Classifier:\n{classification_report(ytest, model_rf_prediction)}')
Classification report for Random Forest Classifier:
              precision    recall  f1-score   support

           0       0.83      0.84      0.83       107
           1       0.62      0.60      0.61        47

    accuracy                           0.77       154
   macro avg       0.72      0.72      0.72       154
weighted avg       0.76      0.77      0.76       154

conf_matrix_rf = confusion_matrix(ytest, model_rf_prediction)
sns.heatmap(conf_matrix_rf, annot=True, fmt='d', cmap='YlGnBu')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for Random forest')
plt.show()

dm=pd.DataFrame({'Actual Values':ytest[30:60],'Predicted Values_rf':model_rf_prediction[30:60]})
dm = dm.reset_index(drop=True)
dm.head(5)
Actual Values	Predicted Values_rf
0	0	0
1	0	0
2	0	0
3	0	0
4	1	1
Hyperparameter tuning for Random Forest

# Define the Random Forest classifier
rf_classifier = RandomForestClassifier()

# Define the hyperparameter grid for grid search
param_grid_rf = {'n_estimators': [50, 100, 200], 'criterion': ['gini', 'entropy'], 'max_depth': [None, 5, 10, 15],
                 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}

# Perform grid search with cross-validation
grid_search_rf = GridSearchCV(rf_classifier, param_grid_rf, cv=5, scoring=None, verbose=1, n_jobs=-1)
grid_search_rf.fit(xtrain,ytrain)

# Get the best hyperparameters from the grid search
best_params_rf = grid_search_rf.best_params_
print("Best Hyperparameters for Random Forest:", best_params_rf)

# Train the Random Forest classifier with the best hyperparameters
best_rf_classifier = RandomForestClassifier(**best_params_rf)
best_rf_classifier.fit(xtrain, ytrain)

# Make predictions on the test set
y_pred_rf = best_rf_classifier.predict(xtest)

# Evaluate the accuracy of the model
accuracy_rf = accuracy_score(ytest, y_pred_rf)
print("Test Accuracy for Random Forest:", accuracy_rf)
Fitting 5 folds for each of 216 candidates, totalling 1080 fits
Best Hyperparameters for Random Forest: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100}
Test Accuracy for Random Forest: 0.7532467532467533
Make a plot of all the algorithm accuracy

classifiers = ['KNN', 'SVM', 'Decision Tree', 'Random Forest']
accuracies = [knn_accuracy_test, svm_accuracy_test, DT_accuracy_test, RF_accuracy_test]

fig, ax = plt.subplots()
bars = plt.bar(classifiers, accuracies, color=['blue', 'orange', 'green', 'red'])
plt.ylabel('Test Accuracy')
plt.title('Test Accuracy for Different Classifiers')
plt.ylim(0, 1)  # Setting y-axis limit between 0 and 1

# Display accuracy values on top of each bar
for bar, acc in zip(bars, accuracies):
    plt.text(bar.get_x() + bar.get_width() / 2 - 0.1, bar.get_height() + 0.02, f'{acc:.2f}',
             ha='center', color='black', fontweight='bold')

plt.show()

From the chart above, we can see that the best performing algorithm is the Random Forest with 0.81 accuracy.
import pickle
 
import pickle
data = {"model": model_knn}
with open('classifier_knn.pkl', 'wb') as file:
    pickle.dump(data, file)
with open('classifier_knn.pkl', 'rb') as file:
    data = pickle.load(file) 
 
 
